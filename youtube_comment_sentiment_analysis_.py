# -*- coding: utf-8 -*-
"""youtube comment sentiment analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14nQurTfO2emX_HYaCEPSgHzcNOxT4tfe
"""

!pip install google-api-python-client
!pip install transformers
!pip install torch
!pip install wordcloud
!pip install plotly
!pip install nltk
!pip install pandas
!pip install matplotlib
!pip install seaborn
!pip install --quiet youtube-comment-downloader

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from transformers import pipeline
from youtube_comment_downloader import YoutubeCommentDownloader

video_url = "https://www.youtube.com/watch?v=4ub10unSjro&list=PLfFghEzKVmjsNtIRwErklMAN8nJmebB0I&index=19"
video_id = video_url.split("v=")[-1]

downloader = YoutubeCommentDownloader()
comments_data = downloader.get_comments_from_url(video_url)

comments = [comment['text'] for comment in comments_data if comment['text']]
df = pd.DataFrame(comments, columns=["Comment"])
df.head()

sentiment_model = pipeline("sentiment-analysis")

df['Sentiment'] = df['Comment'].apply(lambda x: sentiment_model(x)[0]['label'])
df.head()

sns.set(style="whitegrid")
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='Sentiment', palette='Set2')
plt.title("Sentiment Distribution")
plt.show()

text = " ".join(df["Comment"])
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Comment Word Cloud")
plt.show()

df.to_csv("youtube_sentiment_results.csv", index=False)

sentiment_model = pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-sentiment-latest", force_download=True)

sentiment_model = pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-sentiment-latest")

import pandas as pd
import torch
from transformers import pipeline
from googleapiclient.discovery import build
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import plotly.express as px

API_KEY = 'AIzaSyB1UqGu7hylBGE4Bwd09jKMDv8GYdQiR78'
youtube = build('youtube', 'v3', developerKey=API_KEY)

sentiment_pipeline = pipeline("sentiment-analysis")

def get_video_comments(video_id, max_comments=100):
    comments = []
    nextPageToken = None

    while len(comments) < max_comments:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=nextPageToken,
            textFormat="plainText"
        ).execute()

        for item in response["items"]:
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)
            if len(comments) >= max_comments:
                break

        nextPageToken = response.get("nextPageToken")
        if not nextPageToken:
            break

    return comments

def analyze_comments(comments):
    analyzed = []
    for comment in comments:
        try:
            # Skip empty or too-long comments
            if not comment.strip():
                continue
            result = sentiment_pipeline(comment[:512])[0]  # Truncate to 512 characters
            analyzed.append({"Comment": comment, "Sentiment": result["label"]})
        except Exception as e:
            analyzed.append({"Comment": comment, "Sentiment": "ERROR"})
    return pd.DataFrame(analyzed)


def generate_wordcloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

def plot_sentiment_distribution(df):
    fig = px.pie(df, names='Sentiment', title='Sentiment Distribution')
    fig.show()

# üîÅ Main function
def main(video_url, max_comments=100):
    video_id = video_url.split("v=")[-1]
    print("üîç Fetching comments...")
    comments = get_video_comments(video_id, max_comments)
    print(f"‚úÖ Fetched {len(comments)} comments.")

    print("üß† Analyzing sentiment...")
    df = analyze_comments(comments)
    print("üéØ Done.")

    print("üìä Plotting Sentiment Distribution:")
    plot_sentiment_distribution(df)

    print("\nAll Comments:")
    display(df)

    print("\nPositive Comments:")
    display(df[df['Sentiment'] == 'POSITIVE'])

    print("\nNegative Comments:")
    display(df[df['Sentiment'] == 'NEGATIVE'])

    print("‚òÅÔ∏è Generating Word Cloud...")
    generate_wordcloud(" ".join(df["Comment"].tolist()))

    print("üìÅ Exporting CSV...")
    df.to_csv("youtube_sentiment_results.csv", index=False)
    print("‚úÖ CSV saved!")

    return df

pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # Suppress TensorFlow logs
# 
# import streamlit as st
# import pandas as pd
# from transformers import pipeline
# from googleapiclient.discovery import build
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# import plotly.express as px
# 
# st.set_page_config(page_title="YouTube Comment Analyzer", layout="wide")
# 
# # ‚úÖ Replace this with your actual YouTube Data API key
# API_KEY = "AIzaSyB1UqGu7hylBGE4Bwd09jKMDv8GYdQiR78"
# 
# # Initialize YouTube API client
# youtube = build('youtube', 'v3', developerKey=API_KEY)
# 
# # Load sentiment analysis model
# sentiment_pipeline = pipeline("sentiment-analysis")
# def get_video_comments(video_id, max_comments=50):
#     comments = []
#     nextPageToken = None
#     while len(comments) < max_comments:
#         try:
#             response = youtube.commentThreads().list(
#                 part="snippet",
#                 videoId=video_id,
#                 maxResults=100,
#                 pageToken=nextPageToken,
#                 textFormat="plainText"
#             ).execute()
#         except Exception as e:
#             # Check if it's the commentsDisabled error
#             if "commentsDisabled" in str(e):
#                 raise ValueError("üö´ Comments are disabled on this video.")
#             else:
#                 raise e
# 
#         for item in response["items"]:
#             comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
#             comments.append(comment)
#             if len(comments) >= max_comments:
#                 break
# 
#         nextPageToken = response.get("nextPageToken")
#         if not nextPageToken:
#             break
#     return comments
# 
# 
# def analyze_comments(comments):
#     results = sentiment_pipeline(comments)
#     sentiments = [res["label"] for res in results]
#     df = pd.DataFrame({"Comment": comments, "Sentiment": sentiments})
#     return df
# 
# def generate_wordcloud(text):
#     wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
#     plt.figure(figsize=(10, 5))
#     plt.imshow(wordcloud, interpolation="bilinear")
#     plt.axis("off")
#     st.pyplot(plt)
# 
# st.title("üìä YouTube Comment Sentiment Analyzer with Transformers")
# 
# video_url = st.text_input("Enter a YouTube Video URL:")
# max_comments = st.slider("Max number of comments to fetch", 10, 200, 50)
# 
# if st.button("Analyze"):
#     # Extract video ID robustly
#     video_id = None
#     if "v=" in video_url:
#         video_id = video_url.split("v=")[-1].split("&")[0]
#     elif "youtu.be/" in video_url:
#         video_id = video_url.split("youtu.be/")[-1].split("?")[0]
# 
#     if not video_id:
#         st.error("‚ùå Invalid YouTube URL.")
#     else:
#         with st.spinner("Fetching and analyzing comments..."):
#             try:
#                 comments = get_video_comments(video_id, max_comments)
# 
#                 if not comments:
#                     st.warning("No comments found on this video.")
#                 else:
#                     df = analyze_comments(comments)
# 
#                     # ‚úÖ Save CSV to disk automatically
#                     save_path = "sentiment_results_saved.csv"
#                     df.to_csv(save_path, index=False)
#                     st.success(f"‚úÖ Results saved automatically to `{save_path}` in your working directory.")
# 
#                     st.subheader("üìã Sentiment Results")
#                     st.dataframe(df)
# 
#                     st.write("### Positive Comments:")
#                     st.dataframe(df[df["Sentiment"] == "POSITIVE"])
# 
#                     st.write("### Negative Comments:")
#                     st.dataframe(df[df["Sentiment"] == "NEGATIVE"])
# 
#                     st.subheader("üìà Sentiment Distribution")
#                     fig = px.pie(df, names="Sentiment", title="Sentiment Overview")
#                     st.plotly_chart(fig)
# 
#                     st.subheader("‚òÅÔ∏è Word Cloud")
#                     generate_wordcloud(" ".join(df["Comment"]))
# 
#                     # ‚úÖ Download button for convenience
#                     st.download_button(
#                         label="üì• Download Results as CSV",
#                         data=df.to_csv(index=False).encode(),
#                         file_name="sentiment_results.csv",
#                         mime="text/csv"
#                     )
#             except Exception as e:
#                 st.error(f"An error occurred: {e}")
#

!wget -q -O - ipv4.icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501

